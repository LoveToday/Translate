{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载文件\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理西班牙语中的重音 西班牙语中经常会出现带重音的字母，如 á，我们希望把它转换成英文中的 a。 unidecode 库可以将任何 unicode 字符串音译为 ascii 文本中最接近的可能表示形式。 相关代码为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    output = []\n",
    "    for c in unicodedata.normalize('NFD', s):\n",
    "        if unicodedata.category(c) != 'Mn':\n",
    "            output.append(c)\n",
    "    return ''.join(output)\n",
    "#或者 当然也可以采用下面这种更简洁的写法。\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：unicodedata.normalize(‘NFD’, s) 返回标准化后的文本，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Váy:?\n",
      "¿Váy:?\n",
      "7\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "test = u'¿Váy:?'\n",
    "print(unicodedata.normalize('NFD', test))\n",
    "print(test)\n",
    "print(len(unicodedata.normalize('NFD', test)))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然打印出来发现跟没标准化时的一样，但是通过打印长度可以看出，标准化后的文本会比之前的文本多出一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in unicodedata.normalize('NFD', test):\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印标准化文本中的每个单词，我们发现标准化后多出一个单词的原因是它将带重音的单词分成了两个单词。\n",
    "\n",
    "然后，我们使用 unicodedata.category( c ) 来判断这个单词是不是重音，如果是重音的话，它会返回 ‘Mn’，如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in unicodedata.normalize('NFD', test):\n",
    "#     print(c)\n",
    "#     print(unicodedata.category(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "处理ascii文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "    \n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中：\n",
    "\n",
    "w.lower().strip() 将所有字母转换成小写字母； unicode_to_ascii(test.lower().strip()) 去掉重音； re.sub(r\"([?.!,¿])\", r\" \\1 \", w) 在单词与跟在其后的标点符号（\"?\", “.”, “!”, “,”, “¿”）之间插入一个空格； re.sub(r’[\" \"]+’, \" \", w) 将连在一起的多个空格合并为一个空格； re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) 除了 (a-z, A-Z, “.”, “?”, “!”, “,”, “¿”)之外，将所有字符替换为空格； w.strip() 用来去除头尾空格； ’ ’ + w + ’ ’ 给句子加上开始和结束标记，以便模型知道何时开始和结束预测。 仍用之前的 test 举例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = test.lower().strip()\n",
    "# '¿váy:?'\n",
    "w = unicode_to_ascii(w)\n",
    "# '¿vay:?'\n",
    "w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "# ' ¿ vay: ? '\n",
    "w = re.sub(r'[\" \"]+', \" \", w)\n",
    "# ' ¿ vay: ? '\n",
    "w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "# ' ¿ vay ? '\n",
    "w = w.strip()\n",
    "# '¿ vay ?'\n",
    "w = '<start> ' + w + ' <end>'\n",
    "# '<start> ¿ vay ? <end>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为英文中不存在 ‘¿’ 标点符号，所以在处理西班牙语文本时，我们最后要将其编码为 ‘utf-8’ 格式："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5 返回单词对\n",
    "\n",
    "我们需要返回这样格式的单词对：[ENGLISH, SPANISH]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    #strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其中：\n",
    "# io.open(path_to_file, encoding=‘UTF-8’).read() 用来输出编码后的文本；.strip() 用来去掉文本头尾的空格；.split(’\\n’) 使原来的一串字符串按 ‘\\n’ 划分为很多字符串，然后放到一个列表中。\n",
    "\n",
    "# num_examples 表示要返回单词对的对数，当 num_examples=None 时，返回所有单词对。\n",
    "\n",
    "# 比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start> go . <end>', '<start> go . <end>', '<start> go . <end>')\n",
      "('<start> ve . <end>', '<start> vete . <end>', '<start> vaya . <end>')\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, 3)\n",
    "print(en)\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.6 生成文档词典\n",
    "text.Tokenizer 类用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                            padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 fit_on_text(lang) 使用一系列文档来生成 token 词典，lang 是一个列表，每个元素为一个文档。输出的 lang_tokenizer 即为基于以上文档生成的词典。\n",
    "\n",
    "tensor = lang_tokenizer.texts_to_sequences(lang) 将多个文档转换为 word 下标的向量形式，shape 为 [len(texts)，len(text)] 即 (文档数，每条文档的长度)。\n",
    "\n",
    "tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding=‘post’) 的操作是先遍历一遍上一步得到的列表中的每个元素，找出最长的一个元素（即包含的单词最多），然后把所有元素都用 0 进行扩充至最长元素的长度。\n",
    "\n",
    "比如，我们取前25个文本进行实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_lang, inp_lang = create_dataset(path_to_file, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<start> go . <end>',\n",
       " '<start> go . <end>',\n",
       " '<start> go . <end>',\n",
       " '<start> go . <end>',\n",
       " '<start> hi . <end>',\n",
       " '<start> run ! <end>',\n",
       " '<start> run . <end>',\n",
       " '<start> who ? <end>',\n",
       " '<start> fire ! <end>',\n",
       " '<start> fire ! <end>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targ_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "lang_tokenizer.fit_on_texts(targ_lang)\n",
    "tensor1 = lang_tokenizer.texts_to_sequences(targ_lang)\n",
    "tensor2 = tf.keras.preprocessing.sequence.pad_sequences(tensor1, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list,\n",
       " [[1, 4, 3, 2],\n",
       "  [1, 4, 3, 2],\n",
       "  [1, 4, 3, 2],\n",
       "  [1, 4, 3, 2],\n",
       "  [1, 8, 3, 2],\n",
       "  [1, 6, 5, 2],\n",
       "  [1, 6, 3, 2],\n",
       "  [1, 9, 10, 2],\n",
       "  [1, 7, 5, 2],\n",
       "  [1, 7, 5, 2]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensor1),tensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, array([[ 1,  4,  3,  2],\n",
       "        [ 1,  4,  3,  2],\n",
       "        [ 1,  4,  3,  2],\n",
       "        [ 1,  4,  3,  2],\n",
       "        [ 1,  8,  3,  2],\n",
       "        [ 1,  6,  5,  2],\n",
       "        [ 1,  6,  3,  2],\n",
       "        [ 1,  9, 10,  2],\n",
       "        [ 1,  7,  5,  2],\n",
       "        [ 1,  7,  5,  2]], dtype=int32))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensor2), tensor2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以查看生成的词典："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: '.',\n",
       " 4: 'go',\n",
       " 5: '!',\n",
       " 6: 'run',\n",
       " 7: 'fire',\n",
       " 8: 'hi',\n",
       " 9: 'who',\n",
       " 10: '?'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以查看生成的词典："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.7 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # 创建经过处理后的输入输出对\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 9), (1000, 7))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, target_tensor.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8 计算目标张量的最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length_targ, max_length_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.9 划分训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 800 200 200\n"
     ]
    }
   ],
   "source": [
    "# 采用 80 - 20 的比例切分训练集和验证集\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# 显示长度\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.10 将数字向量转化为文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如我们取 input_tensor_train[0] 数字向量，将其输入 convert(lang, tensor) 函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "7 ----> ¿\n",
      "20 ----> me\n",
      "34 ----> puedo\n",
      "778 ----> quedar\n",
      "8 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# input_tensor_train[0] = array([1, 6, 11, 7415, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor_train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.11 创建一个 tf.data 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 9), (64, 7)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val))\n",
    "test_ds = test_ds.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 9), (64, 7)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(822, 373)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_inp_size, vocab_tar_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以查看每次从数据集中取出的样本形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 9]), TensorShape([64, 7]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的1.6-1.11也可以用 tfds.features.text.Tokenizer() 函数来做，大体思路为："
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tokenizer_sp = tfds.features.text.Tokenizer()\n",
    "tokenizer_en = tfds.features.text.Tokenizer()\n",
    "\n",
    "vocabulary_set_sp = set()\n",
    "vocabulary_set_en = set()\n",
    "for each_sp, each_en in zip(sp, en):\n",
    "    some_tokens_en = tokenizer_en.tokenize(each_en)\n",
    "    vocabulary_set_en.update(some_tokens_en)\n",
    "    some_tokens_sp = tokenizer_pt.tokenize(each_sp)\n",
    "    vocabulary_set_sp.update(some_tokens_sp)\n",
    "\n",
    "vocab_size_en = len(vocabulary_set_en)\n",
    "vocab_size_sp = len(vocabulary_set_sp)\n",
    "\n",
    "encoder_en = tfds.features.text.TokenTextEncoder(vocabulary_set_en)\n",
    "encoder_sp = tfds.features.text.TokenTextEncoder(vocabulary_set_sp)\n",
    "\n",
    "def encode(lang):\n",
    "    lang = encoder_sp.encode(lang)\n",
    "    return lang\n",
    "\n",
    "sp_list = []\n",
    "en_list = []\n",
    "for i in range(len(en)):\n",
    "    sp_list.append(encode(sp[i]))\n",
    "    en_list.append(encode(en[i]))\n",
    "\n",
    "sp_list = tf.keras.preprocessing.sequence.pad_sequences(sp_list, padding='post')\n",
    "en_list = tf.keras.preprocessing.sequence.pad_sequences(en_list, padding='post')\n",
    "\n",
    "def tf_encode(sp, en):\n",
    "    result_sp, result_en = tf.py_function(encode, [sp, en], [tf.int64, tf.int64])\n",
    "    result_sp.set_shape([None])\n",
    "    result_en.set_shape([None])\n",
    "\n",
    "    return result_sp, result_en\n",
    "\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_examples = tf.data.Dataset.from_tensor_slices((sp_list, en_list))\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "# 将数据集缓存到内存中以加快读取速度。\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, ((None, ), (None, )))\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 编码器\n",
    "\n",
    "首先，我们将每个单词嵌入 embedding_dim 维的空间向量中，然后输入 GRU 单元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(vocab_inp_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = embedding(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1024])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state =  tf.zeros((64,1024))\n",
    "init_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = tf.keras.layers.GRU(1024,\n",
    "#                                    return_sequences=True,\n",
    "#                                    return_state=True,\n",
    "#                                    recurrent_initializer='glorot_uniform')(x, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在词嵌入层中，样本形状从 (64, 16) 变成 (64, 16, 256)，即每个单词都变成了一个256维的向量。 在含有1024个神经元的 GRU 层中，样本形状又从 (64, 16, 256) 变成 (64, 16, 1024)，GRU 层里的隐藏向量的形状为 (64, 1024)。\n",
    "#输入 (64,23)  经过Embedding 层变为 (64, 23, 256) ，加上一个隐藏层(64,1024)经过GRU层之后变为(64, 23, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 9, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# 样本输入\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Bahdanau 注意力\n",
    "\n",
    "相关论文参考：BahdanauAttention。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "#         print('score.shape=', score.shape)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_with_time_axis = tf.expand_dims(sample_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1, 1024])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_with_time_axis.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = tf.keras.layers.Dense(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 values 其实就是编码器中输出的结果，经过含10个神经元的 Dense 层之后，其形状从 (64, 16, 1024) 变成了 (64, 16, 10)。\n",
    "\n",
    "这里的 query 其实就是编码器中输出的隐层向量，我们需要将其维度从 (64, 1024) 变成 (64, 1， 1024) 来执行之后的加法以计算分数，将增加维度后的向量经过含10个神经元的 Dense 层之后，其形状从 (64, 1, 1024) 变成了 (64, 1, 10)。\n",
    "\n",
    "将以上两个输出相加得到的形状为 (64, 16, 10)，经过含1个神经元的 Dense 层之后得到 score，其形状变成 (64, 16, 1)。\n",
    "\n",
    "Softmax 默认被应用于最后一个轴，但是这里我们想将它应用于第二个轴（即 axis=1），因为分数 （score） 的形状是 (批大小，最大长度，隐藏层大小)。最大长度是我们的输入的长度。因为我们想为每个输入分配一个权重，所以 softmax 应该用在这个轴上。经过 Softmax 层之后，得到的注意力权重形状和 score 的形状相同，都是 (64, 16, 1)。 【注】Softmax 的不同的轴的计算规则参考：tf.nn.softmax(x, axis)里axis起什么作用？\n",
    "\n",
    "将注意力权重和 values 相乘，得到上下文向量，其形状为 (64, 16, 1024)。此向量也就是加了权重的编码向量。将上下文向量基于第二个轴求和（原因与之前相同），得到最终的上下文向量，其形状为 (64, 1024)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=612, shape=(1024,), dtype=float32, numpy=\n",
       "array([ 0.00501692,  0.00456364, -0.00055771, ..., -0.00311722,\n",
       "       -0.00146151, -0.00662348], dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "#         print('x.shape', x.shape)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在解码器中，首先利用注意力层获得上下文向量 (64, 1024) 和注意力权重 (64, 16, 1)。\n",
    "\n",
    "将输入通过词嵌入层，其形状从 (64, 1) 变成了 (64, 1, 256)。\n",
    "\n",
    "在上下文向量上添加一个维度，使它的形状变成 (64, 1, 1024)，然后和通过词嵌入层的输入合并，得到形状为 (64, 1, 1280) 的向量。\n",
    "\n",
    "将合并后的向量传送到 GRU，得到输出向量 (64, 1, 1024) 和隐藏状态 (64, 1024)。\n",
    "\n",
    "将输出向量以最后一维的维数不变的形式转换成二维数组，即形状为 (64*1, 1024)。\n",
    "\n",
    "最后，将这个向量通过一个含有词汇表大小 (4935) 个神经元的全连接层，得到最终输出 (64, 4935)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 373)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  95488     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  multiple                  7084032   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  382325    \n",
      "_________________________________________________________________\n",
      "bahdanau_attention_1 (Bahdan multiple                  2100225   \n",
      "=================================================================\n",
      "Total params: 9,662,070\n",
      "Trainable params: 9,662,070\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3、定义优化器和损失函数\n",
    "\n",
    "每次输入解码器的向量是一个批次中所有文本中的一个单词，即每次输入的向量形状都是 (64, 1)。\n",
    "\n",
    "当输入的向量中出现0元素，说明这个元素所在的文本已经结束了，这个文本不再参与损失的计算，所以在计算损失的时候，要使用掩膜处理，将已结束文本的损失置零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch_loss_avg = tf.keras.metrics.Mean('train_loss')\n",
    "# train_accuracy = tf.keras.metrics.Accuracy()\n",
    "\n",
    "# epoch_loss_avg_test = tf.keras.metrics.Mean('test_loss')\n",
    "# test_accuracy = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    # real= shape=(64,) pred shape=(64, 12512), dtype=float32\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    print('shape', mask.shape)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4、训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1 梯度下降\n",
    "\n",
    "a. 将输入传送至编码器，编码器返回编码器输出和编码器隐藏层状态。 b. 将编码器输出、编码器隐藏层状态和解码器输入（即 ‘’ ）传送至解码器。 c. 解码器返回预测和解码器隐藏层状态。 d. 解码器隐藏层状态被传送回模型，预测被用于计算损失。 e. 使用教师强制（teacher forcing）决定解码器的下一个输入。 PS：教师强制是将目标词作为下一个输入传送至解码器的技术。 f. 最后一步是计算梯度，并将其应用于优化器和反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights('./spanish/encoder_epoch-26.h5', by_name=True)\n",
    "decoder.load_weights('./spanish/decoder_epoch-26.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=976, shape=(), dtype=int32, numpy=1>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    # inp (64, 23)  targ (64, 22)\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # enc_output (64, 23, 1024)  enc_hidden (64, 1024)\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # dec_input (64, 1)\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # predictions (64, 12512)  dec_hidden (64, 1024)\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(inp, targ, enc_hidden):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    print('0000', enc_output.shape, enc_hidden.shape)\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "    for t in range(1, targ.shape[1]):\n",
    "            # predictions (64, 12512)  dec_hidden (64, 1024)\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    return  batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_step(inp, targ, enc_hidden):\n",
    "\n",
    "# for epoch in range(1):\n",
    "#     # (64, 1024)\n",
    "#     enc_hidden = encoder.initialize_hidden_state()\n",
    "#     total_loss = 0\n",
    "#     print(enc_hidden.shape)\n",
    "#     for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "#         print(batch, inp.shape, targ.shape)\n",
    "#         batch_loss = train_step(inp, targ, enc_hidden)\n",
    "#         print('batch_loss=', batch_loss.numpy()) \n",
    "#         total_loss += batch_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 1024)\n",
      "0000 (64, 9, 1024) (64, 1024)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "Epoch 1 val-Loss 0.3310\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    test_total_loss = 0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    print(enc_hidden.shape)\n",
    "    for (batch, (inp, targ)) in enumerate(test_ds.take(1)):\n",
    "        batch_loss = test_step(inp, targ, enc_hidden)\n",
    "        test_total_loss += batch_loss\n",
    "    print('Epoch {} val-Loss {:.4f}'.format(epoch + 1,\n",
    "                                      test_total_loss / steps_per_epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      multiple                  210432    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  3938304   \n",
      "=================================================================\n",
      "Total params: 4,148,736\n",
      "Trainable params: 4,148,736\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Decoder.call of <__main__.Decoder object at 0x136ec7c18>>, which Python reported as:\n",
      "    def call(self, x, hidden, enc_output):\n",
      "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
      "\n",
      "        x = self.embedding(x)\n",
      "#         print('x.shape', x.shape)\n",
      "\n",
      "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
      "        \n",
      "        output, state = self.gru(x)\n",
      "        \n",
      "        output = tf.reshape(output, (-1, output.shape[2]))\n",
      "\n",
      "        x = self.fc(output)\n",
      "\n",
      "        return x, state, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method BahdanauAttention.call of <__main__.BahdanauAttention object at 0x136ed6908>>, which Python reported as:\n",
      "    def call(self, query, values):\n",
      "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
      "\n",
      "        score = self.V(tf.nn.tanh(\n",
      "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
      "#         print('score.shape=', score.shape)\n",
      "\n",
      "        attention_weights = tf.nn.softmax(score, axis=1)\n",
      "\n",
      "        context_vector = attention_weights * values\n",
      "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
      "\n",
      "        return context_vector, attention_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "shape (64,)\n",
      ".Epoch 1 Batch 0 Loss 4.0074\n",
      "...........Epoch 1 Loss 2.4545\n",
      "Time taken for 1 epoch 23.879291772842407 sec\n",
      "\n",
      ".Epoch 2 Batch 0 Loss 1.9790\n",
      "...........Epoch 2 Loss 1.7730\n",
      "Time taken for 1 epoch 10.498432874679565 sec\n",
      "\n",
      ".Epoch 3 Batch 0 Loss 1.5820\n",
      "...........Epoch 3 Loss 1.4960\n",
      "Time taken for 1 epoch 10.113126039505005 sec\n",
      "\n",
      ".Epoch 4 Batch 0 Loss 1.3818\n",
      "...."
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-71c9a6588618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_name = 'spanish'\n",
    "if not os.path.exists('models_'+dataset_name):\n",
    "    os.mkdir('models_'+dataset_name)\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    # 验证总损失\n",
    "    test_total_loss\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        print('.', end='')\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    # 测试集\n",
    "    for (batch, (test_inp, test_targ)) in enumerate(test_ds.take(steps_per_epoch)):\n",
    "        test_batch_loss = test_step(test_inp, test_targ, enc_hidden) \n",
    "        test_total_loss += test_batch_loss\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        encoder.save_weights('models_'+dataset_name+'/encoder_epoch-{}.h5'.format(str(epoch+1)))\n",
    "        decoder.save_weights('models_'+dataset_name+'/decoder_epoch-{}.h5'.format(str(epoch+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5、预测\n",
    "\n",
    "5.1 预测函数\n",
    "\n",
    "1、用 preprocess_sentence 函数处理ascii文本；\n",
    "2、将文本转换成数字向量，并进行填充；\n",
    "3、将数字向量转化为张量；\n",
    "4、初始化编码器的隐藏状态；\n",
    "5、将数字张量和初始化隐藏状态输入编码器；\n",
    "6、初始化解码器输入；\n",
    "7、逐字输入解码器，得到预测的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    \n",
    "    encoder.load_weights('./encoder_epoch-26.h5', by_name=True)\n",
    "    decoder.load_weights('./decoder_epoch-26.h5', by_name=True)\n",
    "    \n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_inp,\n",
    "                                                           padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # 存储注意力权重以便后面制图\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # 预测的 ID 被输送回模型\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 注意力权重制图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 翻译函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Empezamos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Empezamos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'Aún faltan algunas palabras.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
